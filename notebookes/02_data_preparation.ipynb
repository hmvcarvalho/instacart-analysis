{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c0cc090",
   "metadata": {},
   "source": [
    "# Data Preparation - Instacart Market Basket Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook focuses on cleaning and preparing the Instacart datasets for analysis. Based on the findings from our initial data exploration, we will address data quality issues and ensure our datasets are ready for comprehensive analysis.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "**1. Data Quality Assessment**\n",
    "- Handle missing values across all datasets\n",
    "- Remove or flag duplicate records\n",
    "- Validate data consistency and logical constraints\n",
    "\n",
    "**2. Data Cleaning**\n",
    "- Clean inconsistent data formats\n",
    "- Standardize data types where necessary\n",
    "- Address any anomalies identified during exploration\n",
    "\n",
    "**3. Data Integration**\n",
    "- Merge datasets where appropriate for analysis\n",
    "- Create derived variables if needed\n",
    "- Ensure referential integrity between tables\n",
    "\n",
    "**4. Data Validation**\n",
    "- Verify cleaned data meets quality standards\n",
    "- Perform final consistency checks\n",
    "- Document all transformations applied\n",
    "\n",
    "## Expected Outcome\n",
    "\n",
    "Clean, validated datasets ready for in-depth analysis of customer shopping patterns, product preferences, and ordering behaviors.\n",
    "\n",
    "---\n",
    "\n",
    "*Note: All cleaning steps will be documented with clear explanations and rationale for reproducibility.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb25b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dbb245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data to variables\n",
    "df_instacart_orders = pd.read_csv('../data/raw/instacart_orders.csv', sep=';')\n",
    "df_products = pd.read_csv('../data/raw/products.csv', sep=';')\n",
    "df_order_products = pd.read_csv('../data/raw/order_products.csv', sep=';')\n",
    "df_aisles = pd.read_csv('../data/raw/aisles.csv', sep=';')\n",
    "df_departments = pd.read_csv('../data/raw/departments.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88233868",
   "metadata": {},
   "source": [
    "### DataFrame instacart_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1657411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count duplicated orders\n",
    "dup_num = df_instacart_orders.duplicated().sum()\n",
    "message = f\"df_instacart_orders have {dup_num} duplicated lines\"\n",
    "print(message)\n",
    "\n",
    "print(\"===========================================\")\n",
    "#printing duplicated rows sorted by order_id\n",
    "dup_rows = df_instacart_orders[df_instacart_orders.duplicated(keep=False)]\n",
    "print(dup_rows.sort_values(by='order_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfc2910",
   "metadata": {},
   "source": [
    "#### DataFrame instacart_orders Analysis\n",
    "- 15 duplicated orders\n",
    "- All happened on Wednesday (order_dow == 3) at 2am (order_hour_of_day == 2)\n",
    "\n",
    "#### Identified Issues\n",
    "1. Lines are matching several fields (order_id, user_id, order_number)\n",
    "2. The same user_id cannot place the same order_id multiple times\n",
    "3. order_id must be a unique key value (not repeated)\n",
    "\n",
    "#### Conclusion\n",
    "We can conclude this probably happened due to a server failure or an error during backup, so we must remove the duplicated lines. Despite representing a residual number of lines, we must keep data integrity in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee1ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing duplicates and reset index\n",
    "df_instacart_orders_clean = df_instacart_orders.drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "#count duplicated orders\n",
    "dup_num = df_instacart_orders_clean.duplicated().sum()\n",
    "message = f\"df_instacart_orders_cleared have {dup_num} duplicated lines\"\n",
    "print(message)\n",
    "\n",
    "#checking duplicated order_id\n",
    "dup_order_id = df_instacart_orders_clean['order_id'].duplicated().sum()\n",
    "message = f\"df_instacart_orders_cleared have {dup_order_id} duplicated order_id\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77494ef",
   "metadata": {},
   "source": [
    "### DataFrame products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3730346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change product_name to lowercase\n",
    "df_products['product_name'] = df_products['product_name'].str.lower()\n",
    "\n",
    "# verify duplicated lines\n",
    "products_dup_lines = df_products.duplicated().sum()\n",
    "message = f\"products dataframe have {products_dup_lines} duplicated lines\"\n",
    "print(message)\n",
    "\n",
    "# verify duplicated values in column produt_id\n",
    "product_id_dup = df_products['product_id'].duplicated().sum()\n",
    "message = f\"product_id column have {product_id_dup} duplicated values\"\n",
    "print(message)\n",
    "\n",
    "# verify duplicated in product_names\n",
    "product_name_dups = df_products['product_name'].duplicated().sum()\n",
    "message = f\"product_name column have {product_name_dups} duplicated values\"\n",
    "print(message)\n",
    "\n",
    "print(df_products[df_products['product_name'].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c47608",
   "metadata": {},
   "source": [
    "#### DataFrame products Analysis\n",
    "- 0 duplicated full lines (good data quality)\n",
    "- 0 duplicated product_id, good as each product has unique identifier\n",
    "- 1 361 duplicated values in column \"product_name\" due to NaN values\n",
    "\n",
    "#### Identified Issues\n",
    "**Missing Product Names**:\n",
    "   - These all belong to aisle_id = 100 and department_id = 21\n",
    "   - This suggests a systematic data collection issue for products in this specific category\n",
    "   - Likely represents \"missing\" or \"unknown\" products in the system\n",
    "\n",
    "\n",
    "\n",
    "#### Conclusion\n",
    "We can conclude this missing values are related to a especific Aisle (100) /Department(21) so is better to investigate what they represent in the businness. The collection of this especific data must be reviewd. For further analisys the NaN values should be replaced by \"Unknown Product\" or similar placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e2fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the NaN values\n",
    "df_products_clean = df_products[df_products['product_name'].duplicated() == True].fillna('unknown product')\n",
    "\n",
    "#confirm change\n",
    "print(df_products_clean[df_products_clean['product_name'] == 'unknown product'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7b818e",
   "metadata": {},
   "source": [
    "### DataFrame departments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f2ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify duplicated lines\n",
    "departments_dup_lines = df_departments.duplicated().sum()\n",
    "message = f\"departments dataframe have {products_dup_lines} duplicated lines\"\n",
    "print(message)\n",
    "\n",
    "# verify duplicated id\n",
    "department_id_dup = df_departments['department_id'].duplicated().sum()\n",
    "message = f\"departments dataframe have {department_id_dup} duplicated ids\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17607a9",
   "metadata": {},
   "source": [
    "#### DataFrame department Analysis\n",
    "- 0 duplicated full lines (good data quality)\n",
    "- 0 duplicated product_id, good as each department has unique identifier\n",
    "\n",
    "#### Identified Issues\n",
    "**No issues identified**\n",
    "\n",
    "#### Conclusion\n",
    "We can conclude the department list have quality data, but we can also see the problem discovred before with department 21 is the last line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e70bb",
   "metadata": {},
   "source": [
    "### DataFrame aisles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e5d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify duplicated lines\n",
    "aisles_dup_lines = df_aisles.duplicated().sum()\n",
    "message = f\"aisles dataframe have {aisles_dup_lines} duplicated lines\"\n",
    "print(message)\n",
    "\n",
    "# verify duplicated id\n",
    "aisles_id_dup = df_aisles['aisle_id'].duplicated().sum()\n",
    "message = f\"aisles dataframe have {aisles_id_dup} duplicated ids\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd928145",
   "metadata": {},
   "source": [
    "#### DataFrame aisles Analysis\n",
    "- 0 duplicated full lines (good data quality)\n",
    "- 0 duplicated aisle_id, good as each aisle has unique identifier\n",
    "\n",
    "#### Identified Issues\n",
    "**No issues identified**\n",
    "\n",
    "#### Conclusion\n",
    "We can conclude the aisles list have quality data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d58a3",
   "metadata": {},
   "source": [
    "### DataFrame order_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing 'reorder' to type bool\n",
    "df_order_products['reordered'] = df_order_products['reordered'].astype('bool')\n",
    "\n",
    "# changing 'add_to_cart_order' to type int ignoring NaN\n",
    "df_order_products['add_to_cart_order'] = df_order_products['add_to_cart_order'].astype('Int64')\n",
    "\n",
    "# verify duplicated lines\n",
    "order_prod_dup_lines = df_order_products.duplicated().sum()\n",
    "message = f\"order_products dataframe have {order_prod_dup_lines} duplicated lines\"\n",
    "print(message)\n",
    "\n",
    "# min and max values for 'add_to_cart_order'\n",
    "min_orders = df_order_products['add_to_cart_order'].min()\n",
    "max_orders = df_order_products['add_to_cart_order'].max()\n",
    "message = f\"The smallest order have {min_orders} product(s) and the largest have {max_orders} prduct(s)\"\n",
    "print(message)\n",
    "\n",
    "# saving lines with NaN in 'add_to_cart_order'\n",
    "missing_values = df_order_products[df_order_products['add_to_cart_order'].isna() == True]\n",
    "print(missing_values.sort_values(by='order_id'))\n",
    "\n",
    "filtred_df = df_order_products[df_order_products['order_id'] == 61355].sort_values(by='add_to_cart_order')\n",
    "\n",
    "print(filtred_df['add_to_cart_order'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e32e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING CLEANED DATASETS FOR ANALYSIS\n",
    "df_instacart_orders_clean.to_csv('../data/data_clean/instacart_orders_clean.csv', index=False)\n",
    "df_products_clean.to_csv('../data/data_clean/products_clean.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
